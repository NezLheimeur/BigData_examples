{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "needed-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row, SQLContext\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, GBTClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "native-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "##CREATE SPARK CONTEXT#\n",
    "##CREATE SQL CONTEXT###\n",
    "#######################\n",
    "sc =SparkContext()\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "colonial-europe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- variety: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LOAD IRIS DATA\n",
    "\n",
    "data_dir=\".\"\n",
    "file = os.path.join(data_dir,\"iris.csv\")\n",
    "panda_df = pd.read_csv(file)\n",
    "\n",
    "iris_df=sqlContext.createDataFrame(panda_df)\n",
    "iris_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-queens",
   "metadata": {},
   "source": [
    "#### Perform Data Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "portable-native",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+---------+\n",
      "|summary|      sepal_length|       sepal_width|      petal_length|       petal_width|  variety|\n",
      "+-------+------------------+------------------+------------------+------------------+---------+\n",
      "|  count|               150|               150|               150|               150|      150|\n",
      "|   mean| 5.843333333333334|3.0573333333333337|3.7580000000000005|1.1993333333333331|     null|\n",
      "| stddev|0.8280661279778632|0.4358662849366982|1.7652982332594662|0.7622376689603466|     null|\n",
      "|    min|               4.3|               2.0|               1.0|               0.1|   Setosa|\n",
      "|    max|               7.9|               4.4|               6.9|               2.5|Virginica|\n",
      "+-------+------------------+------------------+------------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#See standard parameters\n",
    "iris_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-violin",
   "metadata": {},
   "source": [
    "### Decision tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-flesh",
   "metadata": {},
   "source": [
    "#### Prepare data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "variable-clinton",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform to a Data Frame for input to Machine Learing\n",
    "\n",
    "#stage 1: change the categorical feature to numerical one \n",
    "stringIndexer = StringIndexer(inputCol=\"variety\", outputCol=\"label\")\n",
    "\n",
    "\n",
    "#stage 2: generate one column \"features\" for all the features \n",
    "vectorAssembler = VectorAssembler(inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"],\\\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "#stage 3: create the model \n",
    "classifier_dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "#regroup the stages in a pipeline\n",
    "pipeline_dt = Pipeline(stages=[stringIndexer, vectorAssembler, classifier_dt])\n",
    "\n",
    "# split data\n",
    "(data_train, data_test) = iris_df.randomSplit([0.75, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-database",
   "metadata": {},
   "source": [
    "#### Perform Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "enhanced-smoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb nodes in model dt:  11\n",
      "depth of model dt:  4\n",
      "+----------+-----+-----------------+\n",
      "|prediction|label|         features|\n",
      "+----------+-----+-----------------+\n",
      "|       1.0|  1.0|[4.7,3.2,1.3,0.2]|\n",
      "|       1.0|  1.0|[4.9,3.0,1.4,0.2]|\n",
      "|       1.0|  1.0|[5.4,3.9,1.7,0.4]|\n",
      "|       1.0|  1.0|[4.8,3.4,1.6,0.2]|\n",
      "|       1.0|  1.0|[4.3,3.0,1.1,0.1]|\n",
      "|       1.0|  1.0|[5.7,4.4,1.5,0.4]|\n",
      "|       1.0|  1.0|[5.8,4.0,1.2,0.2]|\n",
      "|       1.0|  1.0|[5.1,3.3,1.7,0.5]|\n",
      "|       1.0|  1.0|[5.1,3.8,1.5,0.3]|\n",
      "|       1.0|  1.0|[5.4,3.4,1.7,0.2]|\n",
      "|       1.0|  1.0|[4.5,2.3,1.3,0.3]|\n",
      "|       1.0|  1.0|[5.0,3.5,1.6,0.6]|\n",
      "|       1.0|  1.0|[5.1,3.8,1.9,0.4]|\n",
      "|       2.0|  2.0|[6.6,2.9,4.6,1.3]|\n",
      "|       2.0|  2.0|[5.6,2.5,3.9,1.1]|\n",
      "|       2.0|  2.0|[5.6,3.0,4.5,1.5]|\n",
      "|       0.0|  2.0|[5.9,3.2,4.8,1.8]|\n",
      "|       2.0|  2.0|[6.1,2.8,4.0,1.3]|\n",
      "|       2.0|  2.0|[6.3,2.5,4.9,1.5]|\n",
      "|       2.0|  2.0|[6.4,2.9,4.3,1.3]|\n",
      "+----------+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test accuracy =  94.5945945945946 %\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  2.0|       0.0|    1|\n",
      "|  1.0|       1.0|   13|\n",
      "|  2.0|       2.0|   12|\n",
      "|  0.0|       0.0|   10|\n",
      "|  0.0|       2.0|    1|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply the pipeline to apply the transformation and train the model \n",
    "model_dt = pipeline_dt.fit(data_train)\n",
    "\n",
    "# make predictions by transforming the features dataframe into predictions dataframe\n",
    "predictions_dt = model_dt.transform(data_test)\n",
    "\n",
    "print(\"nb nodes in model dt: \",model_dt.stages[-1].numNodes)\n",
    "print(\"depth of model dt: \",model_dt.stages[-1].depth)\n",
    "\n",
    "predictions_dt.select(\"prediction\",\"label\", \"features\").show()\n",
    "\n",
    "#Evaluate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \\\n",
    "                    labelCol=\"label\",metricName=\"accuracy\")\n",
    "accuracy_dt = evaluator.evaluate(predictions_dt)    \n",
    "\n",
    "print(\"Test accuracy = \", accuracy_dt*100, \"%\")\n",
    "\n",
    "#Draw a confusion matrix\n",
    "predictions_dt.groupBy(\"label\",\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-playlist",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-thanks",
   "metadata": {},
   "source": [
    "#### Prepare data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gorgeous-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform to a Data Frame for input to Machine Learing\n",
    "\n",
    "#stage 1: as before\n",
    "#stage 2: as before \n",
    "                                  \n",
    "#stage 3: create the model \n",
    "classifier_rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "#regroup the stages in a pipeline\n",
    "pipeline_rf = Pipeline(stages=[stringIndexer, vectorAssembler, classifier_rf])\n",
    "\n",
    "# split data\n",
    "#(data_train, data_test) = iris_df.randomSplit([0.75, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-robin",
   "metadata": {},
   "source": [
    "#### Perform Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "another-laugh",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o281.getParam.\n: java.util.NoSuchElementException: Param bootstrap does not exist.\n\tat org.apache.spark.ml.param.Params.$anonfun$getParam$2(params.scala:729)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.ml.param.Params.getParam(params.scala:729)\n\tat org.apache.spark.ml.param.Params.getParam$(params.scala:727)\n\tat org.apache.spark.ml.PipelineStage.getParam(Pipeline.scala:43)\n\tat sun.reflect.GeneratedMethodAccessor186.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8840ea94e84a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# apply the pipeline to apply the transformation and train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# make predictions by transforming the features dataframe into predictions dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredictions_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_rf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hduser/conda/envs/spark-env/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/hduser/conda/envs/spark-env/lib/python3.7/site-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hduser/conda/envs/spark-env/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/hduser/conda/envs/spark-env/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hduser/conda/envs/spark-env/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0mJava\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hduser/conda/envs/spark-env/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transfer_params_to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mpair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_java_param_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_defaultParamMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mpair_defaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_defaults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hduser/conda/envs/spark-env/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_make_java_param_pair\u001b[0;34m(self, param, value)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolveParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mjava_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0mjava_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hduser/conda/envs/spark-env/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hduser/conda/envs/spark-env/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hduser/conda/envs/spark-env/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o281.getParam.\n: java.util.NoSuchElementException: Param bootstrap does not exist.\n\tat org.apache.spark.ml.param.Params.$anonfun$getParam$2(params.scala:729)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.ml.param.Params.getParam(params.scala:729)\n\tat org.apache.spark.ml.param.Params.getParam$(params.scala:727)\n\tat org.apache.spark.ml.PipelineStage.getParam(Pipeline.scala:43)\n\tat sun.reflect.GeneratedMethodAccessor186.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "# apply the pipeline to apply the transformation and train the model \n",
    "model_rf = pipeline_rf.fit(data_train)\n",
    "\n",
    "# make predictions by transforming the features dataframe into predictions dataframe\n",
    "predictions_rf = model_rf.transform(data_test)\n",
    "\n",
    "print(\"model info : \",model_rf.stages[-1])\n",
    "\n",
    "predictions_rf.select(\"prediction\",\"label\", \"features\").show()\n",
    "\n",
    "#Evaluate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \\\n",
    "                    labelCol=\"label\",metricName=\"accuracy\")\n",
    "accuracy_rf = evaluator.evaluate(predictions_rf)    \n",
    "\n",
    "print(\"Test accuracy = \", accuracy_rf*100, \"%\")\n",
    "\n",
    "#Draw a confusion matrix\n",
    "predictions_rf.groupBy(\"label\",\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaningful-watch",
   "metadata": {},
   "source": [
    "### LogisticRegression Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-occasions",
   "metadata": {},
   "source": [
    "Unfortunatly Pyspark does not handle multi class problems for the gradient boosting. I'll use the logistic regression instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-eligibility",
   "metadata": {},
   "source": [
    "#### Prepare data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pharmaceutical-grave",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform to a Data Frame for input to Machine Learing\n",
    "\n",
    "#stage 1: as before\n",
    "#stage 2: as before \n",
    "                                  \n",
    "#stage 3: create the model \n",
    "classifier_gb = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "#regroup the stages in a pipeline\n",
    "pipeline_gb = Pipeline(stages=[stringIndexer, vectorAssembler, classifier_gb])\n",
    "\n",
    "# split data\n",
    "#(data_train, data_test) = iris_df.randomSplit([0.75, 0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-skill",
   "metadata": {},
   "source": [
    "#### Perform Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "relevant-gilbert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model info :  LogisticRegressionModel: uid=LogisticRegression_b6573008bbf6, numClasses=3, numFeatures=4\n",
      "+----------+-----+-----------------+\n",
      "|prediction|label|         features|\n",
      "+----------+-----+-----------------+\n",
      "|       1.0|  1.0|[4.7,3.2,1.3,0.2]|\n",
      "|       1.0|  1.0|[4.9,3.0,1.4,0.2]|\n",
      "|       1.0|  1.0|[5.4,3.9,1.7,0.4]|\n",
      "|       1.0|  1.0|[4.8,3.4,1.6,0.2]|\n",
      "|       1.0|  1.0|[4.3,3.0,1.1,0.1]|\n",
      "|       1.0|  1.0|[5.7,4.4,1.5,0.4]|\n",
      "|       1.0|  1.0|[5.8,4.0,1.2,0.2]|\n",
      "|       1.0|  1.0|[5.1,3.3,1.7,0.5]|\n",
      "|       1.0|  1.0|[5.1,3.8,1.5,0.3]|\n",
      "|       1.0|  1.0|[5.4,3.4,1.7,0.2]|\n",
      "|       1.0|  1.0|[4.5,2.3,1.3,0.3]|\n",
      "|       1.0|  1.0|[5.0,3.5,1.6,0.6]|\n",
      "|       1.0|  1.0|[5.1,3.8,1.9,0.4]|\n",
      "|       0.0|  2.0|[6.6,2.9,4.6,1.3]|\n",
      "|       0.0|  2.0|[5.6,2.5,3.9,1.1]|\n",
      "|       0.0|  2.0|[5.6,3.0,4.5,1.5]|\n",
      "|       0.0|  2.0|[5.9,3.2,4.8,1.8]|\n",
      "|       0.0|  2.0|[6.1,2.8,4.0,1.3]|\n",
      "|       0.0|  2.0|[6.3,2.5,4.9,1.5]|\n",
      "|       0.0|  2.0|[6.4,2.9,4.3,1.3]|\n",
      "+----------+-----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Test accuracy =  64.86486486486487 %\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  2.0|       0.0|   13|\n",
      "|  1.0|       1.0|   13|\n",
      "|  0.0|       0.0|   11|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply the pipeline to apply the transformation and train the model \n",
    "model_gb = pipeline_gb.fit(data_train)\n",
    "\n",
    "# make predictions by transforming the features dataframe into predictions dataframe\n",
    "predictions_gb = model_gb.transform(data_test)\n",
    "\n",
    "print(\"model info : \",model_gb.stages[-1])\n",
    "\n",
    "predictions_gb.select(\"prediction\",\"label\", \"features\").show()\n",
    "\n",
    "#Evaluate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \\\n",
    "                    labelCol=\"label\",metricName=\"accuracy\")\n",
    "accuracy_gb = evaluator.evaluate(predictions_gb)    \n",
    "\n",
    "print(\"Test accuracy = \", accuracy_gb*100, \"%\")\n",
    "\n",
    "#Draw a confusion matrix\n",
    "predictions_gb.groupBy(\"label\",\"prediction\").count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
